# 多线程爬虫
目的是爬到板块内所有的帖子标题以及帖子内回复。板块内翻页链接基本不变，只是后面多了`p=*`的部分，
例如a板块第二页：

http://blabla&p=2

所以板块内的链接将`p=*` 循环就好，版块内帖子的链接需要在该页面爬取，整个的思路为：
  1. 取到所有帖子的链接
  2. 爬取链接内容

## 主要用到的包

    import re  #正则匹配
    import requests  #打开网页
    import multiprocessing as mp  #多线程
    from bs4 import BeautifulSoup #解析网页
## 登陆 解析网页
会话对象(session)让你能够跨请求保持某些参数。它也会在同一个 Session 实例发出的所有请求之间保持 cookie

    s = requests.session()  
    s.trust_env = False # faster  
    s.keep_alive = False
    res = s.post(login_url, params=params) #params里有账号密码等
    cookie = res.cookies  # 记下cookie，之后get命令直接用这个cookie
    s.get(url, cookies=cookie) 

  会话还可以用作前后文管理器，这样就能确保 `with` 区块退出后会话能被关闭，即使发生了异常也一样。

  使用 `r.content` 来找到编码，然后设置 `r.encoding` 为相应的编码

  使用BeautifulSoup将网页解析成 lxml


    with s.get(url, cookies=cookie) as r:  
        if r.status_code == 200:  
            time.sleep(0.1)  #停一会
            page_soup = BeautifulSoup(r.content.decode('utf-8', 'ignore'), "lxml")  

## 爬取网页内容
以爬取页面内帖子链接为例，主要用BeautifulSoup里的find_all, 最终结果放在link里

chrome浏览器找到想爬的数据 右键->检查

由于页面中有不同类的链接 `re.compile`正则取到想要的链接


    find_page = blog_page_soup.find_all('a',
    {'href':re.compile('act=index.*id=(\d+)'),  
      'target':'_blank',  
      'title':pattern_ch_en  
    })  
    for i in find_page: link.append(i['href'])
## 多线程
多线程用的是`multiprocessing` `apply_async()`只能输入一组参数，结果用`get()`取出

    import multiprocessing as mp
    pool = mp.Pool(7)  #定义CPU核数
    link_out = [pool.apply_async(get_page_links, args=(page,)) for page in range(ps,pe+1)] 
    link_list = [l.get() for l in link_out]





